<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>THETA 360 Developers Unofficial Blog</title>
    <description>RICOH THETA developer community blog. Tips on using the RICOH THETA S  360 degree camera. The API is based on Open Spherical Camera (OSC) specification 
</description>
    <link>http://theta360developers.github.io/blog/</link>
    <atom:link href="http://theta360developers.github.io/blog/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Tue, 22 Dec 2015 12:57:05 -0800</pubDate>
    <lastBuildDate>Tue, 22 Dec 2015 12:57:05 -0800</lastBuildDate>
    <generator>Jekyll v3.0.1</generator>
    
      <item>
        <title>Using RICOH THETA Live View With Unity</title>
        <description>&lt;p&gt;&lt;a href=&quot;http://tips.hecomi.com/about&quot;&gt;hecomi&lt;/a&gt; recently wrote an &lt;a href=&quot;http://tips.hecomi.com/entry/2015/10/11/211456&quot;&gt;interesting blog post&lt;/a&gt; using Unity to view realtime 360 degree video streaming. I personally have very little experience with Unity, but the content and pictures were useful, so I translated the blog post for others to use. This is not an exact translation, but it should be much more clear than doing Google translate.&lt;/p&gt;

&lt;p&gt;I noticed GOROman (&lt;a href=&quot;https://twitter.com/GOROman&quot;&gt;@GOROman&lt;/a&gt;) tweeting about the info below, so I decided to try it myself right then and there.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://twitter.com/GOROman/status/645896791469068288?ref_src=twsrc%5Etfw&quot;&gt;@GOROman tweet&lt;/a&gt;: You should be able to easily build a live VR stream from this. Stitching might be an issue… For the time being, it might be fine to just connect the sphere to the UV texture.&lt;/p&gt;

&lt;p&gt;The THETA S is a 360 degree camera that will be going on sale October 23rd, and it includes features that were not included in the m15 like Dual-Eye Fisheye streaming over USB (1280x720 15fps) and HDMI streaming (1920x1080 30fps). In order to view this using Unity, I made a an appropriately UV developed sphere and a shader appropriate to AlphaBlend border. Ultimately, for the purpose of making a full sphere with the THETA S, it would be much higher quality and more convenient (can use Skybox too!) to use the fragment shader, made by Nora (&lt;a href=&quot;https://twitter.com/stereoarts&quot;&gt;@Stereoarts&lt;/a&gt;, which directly writes Equirectangular onto a plane.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://stereoarts.jp/&quot;&gt;Stereoarts Homepage&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://twitter.com/Stereoarts/status/647737666520248321?ref_src=twsrc%5Etfw&quot;&gt;@Stereoarts tweet&lt;/a&gt;: I’ve released a Theta Shader Pack. A shader for converting THETA / THETA S full sphere video to Enquirectangular in Unity and supporting scripts. &lt;a href=&quot;stereoarts.jp/ThetaShaderPack_20150926.zip&quot;&gt;stereoarts.jp/ThetaShaderPack_20150926.zip&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For this article, I wanted to jot down my techniques as well.&lt;/p&gt;

&lt;p&gt;Sample&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://dl.dropboxusercontent.com/u/7131835/Programs/ThetaS_LiveView_Sample.unitypackage&quot;&gt;ThetaS_LiveView_Sample.unitypackage @ Dropbox&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Example of taking a video with THETA&lt;/p&gt;

&lt;p&gt;When taking a video with m15, the circles overlapped. The THETA S gives beautifully separated spheres. The angle covered in one sphere is slightly larger than 180 degrees.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/img/2015-12/dual-fish-eye.jpg&quot; alt=&quot;dual fish eye image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/img/2015-12/dual-fisheye-tripod.jpg&quot; alt=&quot;dual fish eye image with tripod&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In doing this, I’ve made an adjustment using sample texture, which Goroman filmed using WebCamTexture.&lt;/p&gt;

&lt;p&gt;Making a sphere with good UV setting&lt;/p&gt;

&lt;p&gt;Working with Maya LT, the UV comes out like this, if you make a normal sphere.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/img/2015-12/MayaLT-UV-mapping.png&quot; alt=&quot;Maya LT UV Mapping&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It would look like below, if you make a plane with the UV.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/img/2015-12/MayaLT-UV-mapping-2.png&quot; alt=&quot;Maya LT UV Mapping 2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;All it needs is to be cut in half and be moved appropriately.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/img/2015-12/MayaLT-UV-mapping-3.png&quot; alt=&quot;Maya LT UV Mapping 3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/img/2015-12/MayaLT-UV-mapping-4.png&quot; alt=&quot;Maya LT UV Mapping 4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It looks like this. (I did not adjust it, so it might be slightly off.)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/img/2015-12/crescent-moon.png&quot; alt=&quot;Crescent Moon image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Actually, I wanted to use alphablend for the border, so I used 2 overlapping half spheres instead of one sphere. The UV border is adequately stretched manually.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/img/2015-12/overlapping.png&quot; alt=&quot;Overlapping&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Incidentally, surface is set to face inward, by reversing all normal vectors. UV position and size are fine to adjust later with shader.&lt;/p&gt;

&lt;p&gt;Setting with Unity&lt;/p&gt;

&lt;p&gt;Import the Maya LT built model with Unity, and put the camera in the center. Write a shader, so the model’s UV position can be adjusted or can alphablend. In order to control the drawing order and to prevent the border from changing at certain orientations, each half sphere has a different shader.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Shader &quot;Theta/Sphere1&quot; {
    Properties {
        _MainTex (&quot;Base (RGB)&quot;, 2D) = &quot;white&quot; {}
        _AlphaBlendTex (&quot;Alpha Blend (RGBA)&quot;, 2D) = &quot;white&quot; {}
        _OffsetU (&quot;Offset U&quot;, Range(-0.5, 0.5)) = 0
        _OffsetV (&quot;Offset V&quot;, Range(-0.5, 0.5)) = 0
        _ScaleU (&quot;Scale U&quot;, Range(0.8, 1.2)) = 1
        _ScaleV (&quot;Scale V&quot;, Range(0.8, 1.2)) = 1
        _ScaleCenterU (&quot;Scale Center U&quot;, Range(0.0, 1.0)) = 0
        _ScaleCenterV (&quot;Scale Center V&quot;, Range(0.0, 1.0)) = 0
    }
    SubShader {
        Tags { &quot;RenderType&quot; = &quot;Transparent&quot; &quot;Queue&quot; = &quot;Background&quot; }
        Pass {
            Name &quot;BASE&quot;

            Blend SrcAlpha OneMinusSrcAlpha
            Lighting Off
            ZWrite Off

            CGPROGRAM
            #pragma vertex vert_img
            #pragma fragment frag

            #include &quot;UnityCG.cginc&quot;

            uniform sampler2D _MainTex;
            uniform sampler2D _AlphaBlendTex;
            uniform float _OffsetU;
            uniform float _OffsetV;
            uniform float _ScaleU;
            uniform float _ScaleV;
            uniform float _ScaleCenterU;
            uniform float _ScaleCenterV;

            float4 frag(v2f_img i) : COLOR {
                // 中心位置や大きさを微調整
                float2 uvCenter = float2(_ScaleCenterU, _ScaleCenterV);
                float2 uvOffset = float2(_OffsetU, _OffsetV);
                float2 uvScale = float2(_ScaleU, _ScaleV);
                float2 uv =  (i.uv - uvCenter) * uvScale + uvCenter + uvOffset;
                // アルファブレンド用のテクスチャを参照してアルファを調整
                float4 tex = tex2D(_MainTex, uv);
                tex.a *= pow(1.0 - tex2D(_AlphaBlendTex, i.uv).a, 2);
                return tex;
            }
            ENDCG
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Here’s a second section of code.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;	Shader &quot;Theta/Sphere2&quot; {
	    Properties {
	        _MainTex (&quot;Base (RGB)&quot;, 2D) = &quot;white&quot; {}
	        _AlphaBlendTex (&quot;Alpha Blend (RGBA)&quot;, 2D) = &quot;white&quot; {}
	        _OffsetU (&quot;Offset U&quot;, Range(-0.5, 0.5)) = 0
	        _OffsetV (&quot;Offset V&quot;, Range(-0.5, 0.5)) = 0
	        _ScaleU (&quot;Scale U&quot;, Range(0.8, 1.2)) = 1
	        _ScaleV (&quot;Scale V&quot;, Range(0.8, 1.2)) = 1
	        _ScaleCenterU (&quot;Scale Center U&quot;, Range(0.0, 1.0)) = 0
	        _ScaleCenterV (&quot;Scale Center V&quot;, Range(0.0, 1.0)) = 0
	    }
	    SubShader {
	        Tags { &quot;RenderType&quot; = &quot;Transparent&quot; &quot;Queue&quot; = &quot;Background+1&quot; }
	        UsePass &quot;Theta/Sphere1/BASE&quot;
	    }
	}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;As below, for alphablend, have a texture made, that is alpha adjusted to UV. I made adjustment for perfectly fit, by exporting UV with postscript and reading with illustrator (white circle inside is alpha=1; around the circle, from inside to outside, changes from 1 to 0; outside will not be used so whatever fits.)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/img/2015-12/two-circles.png&quot; alt=&quot;Two Circles&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Then, adjust the parameters and you’ve got a whole sphere.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/img/2015-12/parameters.png&quot; alt=&quot;Parameters&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/img/2015-12/sphere-unity.png&quot; alt=&quot;Unity Sphere&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/img/2015-12/realtime-stitching.png&quot; alt=&quot;Realtime Stitching&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Changing into Equirectangular&lt;/p&gt;

&lt;p&gt;I tried it with a modified vertex shader.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Shader &quot;Theta/Equirectangular1&quot; {
    Properties {
        _MainTex (&quot;Base (RGB)&quot;, 2D) = &quot;white&quot; {}
        _AlphaBlendTex (&quot;Alpha Blend (RGBA)&quot;, 2D) = &quot;white&quot; {}
        _OffsetU (&quot;Offset U&quot;, Range(-0.5, 0.5)) = 0
        _OffsetV (&quot;Offset V&quot;, Range(-0.5, 0.5)) = 0
        _ScaleU (&quot;Scale U&quot;, Range(0.8, 1.2)) = 1
        _ScaleV (&quot;Scale V&quot;, Range(0.8, 1.2)) = 1
        _ScaleCenterU (&quot;Scale Center U&quot;, Range(0.0, 1.0)) = 0
        _ScaleCenterV (&quot;Scale Center V&quot;, Range(0.0, 1.0)) = 0
        _Aspect (&quot;Aspect&quot;, Float) = 1.777777777
    }
    SubShader {
        Tags { &quot;RenderType&quot; = &quot;Transparent&quot; &quot;Queue&quot; = &quot;Background&quot; }
        Pass {
            Name &quot;BASE&quot;

            Blend SrcAlpha OneMinusSrcAlpha
            Lighting Off
            ZWrite Off

            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag
            #define PI 3.1415925358979

            #include &quot;UnityCG.cginc&quot;

            uniform sampler2D _MainTex;
            uniform sampler2D _AlphaBlendTex;
            uniform float _OffsetU;
            uniform float _OffsetV;
            uniform float _ScaleU;
            uniform float _ScaleV;
            uniform float _ScaleCenterU;
            uniform float _ScaleCenterV;
            uniform float _Aspect;

            struct v2f {
                float4 position : SV_POSITION;
                float2 uv       : TEXCOORD0;
            };

            v2f vert(appdata_base v) {
                float4 modelBase = mul(_Object2World, float4(0, 0, 0, 1));
                float4 modelVert = mul(_Object2World, v.vertex);

                float x = modelVert.x;
                float y = modelVert.y;
                float z = modelVert.z;

                float r = sqrt(x*x + y*y + z*z);
                x /= 2 * r;
                y /= 2 * r;
                z /= 2 * r;

                float latitude  = atan2(0.5, -y);
                float longitude = atan2(x, z);  

                float ex = longitude / (2 * PI);
                float ey = (latitude - PI / 2) / PI * 2;
                float ez = 0;

                ex *= _Aspect;

                modelVert = float4(float3(ex, ey, ez) * 2 * r, 1);

                v2f o;
                o.position = mul(UNITY_MATRIX_VP, modelVert);
                o.uv       = MultiplyUV(UNITY_MATRIX_TEXTURE0, v.texcoord);
                return o;
            }    

            float4 frag(v2f i) : COLOR {
                float2 uvCenter = float2(_ScaleCenterU, _ScaleCenterV);
                float2 uvOffset = float2(_OffsetU, _OffsetV);
                float2 uvScale = float2(_ScaleU, _ScaleV);
                float2 uv =  (i.uv - uvCenter) * uvScale + uvCenter + uvOffset;
                float4 tex = tex2D(_MainTex, uv);
                tex.a *= pow(1.0 - tex2D(_AlphaBlendTex, i.uv).a, 2);
                return tex;
            }
            ENDCG
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Here’s a second section of code.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Shader &quot;Theta/Equirectangular2&quot; {
    Properties {
        _MainTex (&quot;Base (RGB)&quot;, 2D) = &quot;white&quot; {}
        _AlphaBlendTex (&quot;Alpha Blend (RGBA)&quot;, 2D) = &quot;white&quot; {}
        _OffsetU (&quot;Offset U&quot;, Range(-0.5, 0.5)) = 0
        _OffsetV (&quot;Offset V&quot;, Range(-0.5, 0.5)) = 0
        _ScaleU (&quot;Scale U&quot;, Range(0.8, 1.2)) = 1
        _ScaleV (&quot;Scale V&quot;, Range(0.8, 1.2)) = 1
        _ScaleCenterU (&quot;Scale Center U&quot;, Range(0.0, 1.0)) = 0
        _ScaleCenterV (&quot;Scale Center V&quot;, Range(0.0, 1.0)) = 0
        _Aspect (&quot;Aspect&quot;, Float) = 1.777777777
    }
    SubShader {
        Tags { &quot;RenderType&quot; = &quot;Transparent&quot; &quot;Queue&quot; = &quot;Background+1&quot; }
        UsePass &quot;Theta/Equirectangular1/BASE&quot;
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/blog/img/2015-12/results.png&quot; alt=&quot;Results&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When taking a look at the mesh, it moves around like this.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/img/2015-12/results-mesh.png&quot; alt=&quot;Results Mesh&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Because polygon did not fit, there is a blank space in the corner. This could have been avoided if we have used a direct fragment shader like Nora.&lt;/p&gt;

&lt;p&gt;Conclusion&lt;/p&gt;

&lt;p&gt;It looks like there’s the possibility of multiple fun topics here like spherical AR and Stabilization. After the THETA S goes on sale, I would love to play with it more.&lt;/p&gt;
</description>
        <pubDate>Thu, 17 Dec 2015 00:00:00 -0800</pubDate>
        <link>http://theta360developers.github.io/blog/unity/2015/12/17/Using-RICOH-Live-View-With-Unity.html</link>
        <guid isPermaLink="true">http://theta360developers.github.io/blog/unity/2015/12/17/Using-RICOH-Live-View-With-Unity.html</guid>
        
        
        <category>Unity</category>
        
      </item>
    
      <item>
        <title>Using Node.js with the RICOH THETA S</title>
        <description>&lt;p&gt;&lt;a href=&quot;http://qiita.com/FePlus&quot;&gt;Satoru Yamada&lt;/a&gt; recently wrote a
&lt;a href=&quot;http://qiita.com/FePlus/items/aaeca40468d49786e2f5&quot;&gt;blog post describing how to access the RICOH THETA S with
node.js&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Since I’ve been trying to test the THETA API with JavaScript,
I spent a few minutes to translate his main points into
English.&lt;/p&gt;

&lt;p&gt;I’ll start off Satoru’s code snippet. The rest of the text
is also from him, including the summary, which includes an
interesting idea about NFC tags to help with automation.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;var fs = require(&#39;fs&#39;);
var OscClientClass = require(&#39;osc-client&#39;).OscClient;

var domain = &#39;192.168.1.1&#39;;
var port = &#39;80&#39;;
var client = new OscClientClass(domain, port);
var sessionId;
var filename;

client.startSession().then(function(res){
  sessionId = res.body.results.sessionId;
  return client.takePicture(sessionId);
  })

.then(function (res) {
  var pictureUri = res.body.results.fileUri;
  console.log(&#39;pictureUri :%s&#39;,pictureUri);

  var path = pictureUri.split(&#39;/&#39;);
  filename = path.pop();
  return client.getImage(pictureUri);
})
.then(function(res){
  var imgData = res.body;
  fs.writeFile(filename,imgData);
  return client.closeSession(sessionId);
});
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;By running the &lt;code class=&quot;highlighter-rouge&quot;&gt;node theta.js&lt;/code&gt;, you can view the &lt;code class=&quot;highlighter-rouge&quot;&gt;fileUri&lt;/code&gt;
of the picture that you previously took.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;node theta.js
pictureUri :100RICOH/R0010009.JPG
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The file will be created in the directory where you executed
the command.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/img/2015-12/directory_screenshot.png&quot; alt=&quot;Directory with file&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The IP address and port are described in the &lt;a href=&quot;https://developers.theta360.com/en/docs/v2/api_reference/&quot;&gt;API reference&lt;/a&gt;.
With a P2P connection, only one client can connect.&lt;/p&gt;

&lt;p&gt;An overview of the process is shown below:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;set up a new session with &lt;code class=&quot;highlighter-rouge&quot;&gt;startSession()&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;takePicture&lt;/code&gt; using the &lt;code class=&quot;highlighter-rouge&quot;&gt;sessionId&lt;/code&gt; you just got from the new session&lt;/li&gt;
  &lt;li&gt;use the &lt;code class=&quot;highlighter-rouge&quot;&gt;fileUri&lt;/code&gt; you got from &lt;code class=&quot;highlighter-rouge&quot;&gt;takePicture&lt;/code&gt; to download the image with &lt;code class=&quot;highlighter-rouge&quot;&gt;getImage()&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;close the session with &lt;code class=&quot;highlighter-rouge&quot;&gt;closeSession()&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;
&lt;p&gt;Since the new THETA S supports Open Spherical Camera API, it’s now possible
to easily build applications. As sales and thus the popularity of the
camera appear to be going quite well, it seems likely that we’ll see
many interesting uses in the future.&lt;/p&gt;

&lt;p&gt;Personally, I’ve been thinking that since it may be difficult to turn the
WiFi on and off, start THETA applications, and perform repetitive tasks,
using a NFC tag might help with the automation. If I finish that,
I’ll write another post.&lt;/p&gt;
</description>
        <pubDate>Thu, 17 Dec 2015 00:00:00 -0800</pubDate>
        <link>http://theta360developers.github.io/blog/javascript/2015/12/17/theta-s-nodejs.html</link>
        <guid isPermaLink="true">http://theta360developers.github.io/blog/javascript/2015/12/17/theta-s-nodejs.html</guid>
        
        
        <category>javascript</category>
        
      </item>
    
      <item>
        <title>RICOH THETA Developer Community Services</title>
        <description>&lt;p&gt;The RICOH THETA S is an extremely exciting product. We’re at the beginning
of a larger movement for the general public to use 360 images and 360 videos.
Every week, I learn new things about what major companies are doing
with 360 images and videos. Facebook, YouTube, Google, The New York Times, are
just a few of many organizations that are now supporting 360 media.&lt;/p&gt;

&lt;p&gt;My colleague, Jesse Casman, and I are helping RICOH with technical
marketing activities in the US. Although, we are outside consultants,
we feel like we’re part of a much larger community that includes RICOH,
independent developers, enthusiasts, companies, and frankly, anyone
that thinks that 360 pictures and videos are cool.&lt;/p&gt;

&lt;p&gt;Although we are technical marketers, we’re super enthusiastic about
developers and even though we have limited resources, we’re are trying
to figure out what we can do to help. Feedback from developers is important. The services
are for you.&lt;/p&gt;

&lt;p&gt;Here is our plan to provide basic help to you with our limited resources and
limited technical knowledge.&lt;/p&gt;

&lt;h2 id=&quot;forumhttpsdeveloperstheta360comenforums&quot;&gt;&lt;a href=&quot;https://developers.theta360.com/en/forums/&quot;&gt;Forum&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Jesse and I will go into the forum and answer questions when  we can. For the
questions that we can’t answer, we’ll rely on the community. If the question
relates to a broad group of people, we will try and replicate the problem
and then report back to RICOH engineers. An example of a problem we’ll
escalate is a problem with an application due to changes in Adobe Air.&lt;/p&gt;

&lt;p&gt;There are certain requests for information that RICOH can’t provide. For example,
RICOH can’t release lens parameter information or stitching library algorithms.
For these questions, we will respond that RICOH can’t provide the information
and organize the requests so that RICOH managers have an understand of
what the community is requesting.&lt;/p&gt;

&lt;h2 id=&quot;github-repository-managementhttpsgithubcomtheta360developers&quot;&gt;&lt;a href=&quot;https://github.com/theta360developers&quot;&gt;GitHub Repository Management&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;We will organize code examples from the community. The examples cover both
the control of the camera as well as basic functionality such as viewing a
equirectangular image.&lt;/p&gt;

&lt;p&gt;Over time, we will either test the code ourselves or get help from the community
in testing the code. If the listing becomes too large, we will organize the
repositories into sub-directories and categories.&lt;/p&gt;

&lt;h2 id=&quot;bloghttptheta360developersgithubioblog&quot;&gt;&lt;a href=&quot;http://theta360developers.github.io/blog/&quot;&gt;Blog&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;There’s a lot of great content out there about using the RICOH THETA S.
Some of it is in Japanese and a lot of it is in English.
The developer community in Japan is very active. Jesse can
read Japanese quickly and
I can struggle through it. We’ll combine content from Japan with
English content and provide articles in English about viewing and
streaming content, controlling the camera with the API and using
THETA products and media with other tools or services from companies
like Google, Facebook, Oculus, and Unity.&lt;/p&gt;

&lt;h2 id=&quot;community-guidehttptheta360developersgithubiocommunity-documentcommunityhtml&quot;&gt;&lt;a href=&quot;http://theta360developers.github.io/community-document/community.html&quot;&gt;Community Guide&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;We’re building a community guide based on questions and responses about the
most common topics. Hopefully, we’ll be able to pull popular topics from the blog.&lt;/p&gt;

</description>
        <pubDate>Thu, 17 Dec 2015 00:00:00 -0800</pubDate>
        <link>http://theta360developers.github.io/blog/community/2015/12/17/theta-s-community-assistance.html</link>
        <guid isPermaLink="true">http://theta360developers.github.io/blog/community/2015/12/17/theta-s-community-assistance.html</guid>
        
        
        <category>community</category>
        
      </item>
    
  </channel>
</rss>
